{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4cba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559dbdcf",
   "metadata": {},
   "source": [
    "# WITH TWITTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e6784d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('dataset/traffic_label_acceptable_traffic_data_new.csv')\n",
    "\n",
    "day_of_week = pd.date_range('21/02/2021', '08/07/2021', freq='D').to_series().dt.dayofweek\n",
    "raw_data['weekday'] = 0\n",
    "\n",
    "datetimes = raw_data['recorded'].to_numpy()\n",
    "days = np.core.defchararray.split(datetimes.astype(str), sep=' ')\n",
    "\n",
    "unique_dates = dict()\n",
    "\n",
    "for day in days:\n",
    "    if unique_dates.get(day[0]) == None:\n",
    "        unique_dates[day[0]] = True\n",
    "\n",
    "valid_days = list(unique_dates.keys())\n",
    "\n",
    "for day in unique_dates:\n",
    "    raw_data.loc[raw_data['recorded'].str.contains(day), 'weekday'] = day_of_week[day]\n",
    "    \n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "raw_data[['ds1', 'mf1', 'rf1', 'ds2', 'mf2', 'rf2', 'ds3', 'mf3', 'rf3', 'num_neg', 'num_pos']] = scaler.fit_transform(raw_data[['ds1', 'mf1', 'rf1', 'ds2', 'mf2', 'rf2', 'ds3', 'mf3', 'rf3', 'num_neg', 'num_pos']])\n",
    "\n",
    "raw_data['nextday_weekday'] = 0\n",
    "raw_data['nextday_weekend'] = 0\n",
    "raw_data['nextday_holiday'] = 0\n",
    "\n",
    "old_holiday = ('4/2/2021', '4/3/2021', '4/4/2021', '4/5/2021', '4/26/2021', '4/25/2021', '3/30/2021', '3/31/2021', '4/1/2021')\n",
    "holiday = set()\n",
    "\n",
    "for day in old_holiday:\n",
    "    date = pd.to_datetime(day, format='%m/%d/%Y')\n",
    "    holiday.add(date.strftime('%Y-%m-%d'))\n",
    "    \n",
    "for day in unique_dates.keys():\n",
    "    if (day_of_week[day] + 1) % 7 == 5 or (day_of_week[day] + 1) % 7 == 6:\n",
    "        raw_data.loc[raw_data['recorded'].str.contains(day), 'nextday_weekend'] = 1\n",
    "    \n",
    "    if (day_of_week[day] + 1) % 7 >= 0 and (day_of_week[day] + 1) % 7 <= 4:\n",
    "        raw_data.loc[raw_data['recorded'].str.contains(day), 'nextday_weekday'] = 1\n",
    "\n",
    "    if (pd.to_datetime(day) + dt.timedelta(days=1)).strftime('%Y-%m-%d') in holiday:\n",
    "        raw_data.loc[raw_data['recorded'].str.contains(day), 'nextday_holiday'] = 1\n",
    "        \n",
    "datetime = pd.to_datetime(np.array(list(unique_dates.keys()))).sort_values().strftime('%Y-%m-%d')\n",
    "datetime = datetime.drop('2021-06-11')\n",
    "\n",
    "training_datetime = np.array([['2021-02-21', '2021-02-22', '2021-02-23', '2021-02-24', '2021-02-25', '2021-02-26', '2021-02-27', '2021-02-28', '2021-03-01', '2021-03-02', '2021-03-03'],\n",
    "       ['2021-03-12', '2021-03-13', '2021-03-14', '2021-03-15', '2021-03-16', '2021-03-17', '2021-03-18', '2021-03-19', '2021-03-20', '2021-03-21', '2021-03-22', '2021-03-23'],\n",
    "       ['2021-03-26', '2021-03-27', '2021-03-28', '2021-03-29'], \n",
    "       ['2021-04-04', '2021-04-05', '2021-04-06', '2021-04-07', '2021-04-08', '2021-04-09'],\n",
    "       ['2021-04-13', '2021-04-14', '2021-04-15', '2021-04-16', '2021-04-17', '2021-04-18'],\n",
    "       ['2021-04-23', '2021-04-24', '2021-04-25', '2021-04-26', '2021-04-27', '2021-04-28', '2021-04-29', '2021-04-30', '2021-05-01'],\n",
    "       ['2021-05-04', '2021-05-05', '2021-05-06', '2021-05-07']], dtype=object)\n",
    "\n",
    "validation_datetime = np.array([['2021-05-08', '2021-05-09', '2021-05-10', '2021-05-11', '2021-05-12', '2021-05-13', '2021-05-14'],\n",
    "        ['2021-05-21', '2021-05-22', '2021-05-23', '2021-05-24', '2021-05-25', '2021-05-26', '2021-05-27'],\n",
    "        ['2021-06-01', '2021-06-02', '2021-06-03']], dtype=object)\n",
    "\n",
    "test_datetime = np.array([['2021-06-04', '2021-06-05', '2021-06-06', '2021-06-07'], \n",
    "                          ['2021-06-15', '2021-06-16', '2021-06-17'],\n",
    "                         ['2021-03-30', '2021-03-31', '2021-04-01', '2021-04-02', '2021-04-03']], dtype=object)\n",
    "\n",
    "\n",
    "training_data = dict()\n",
    "validation_data = dict()\n",
    "testing_data = dict()\n",
    "\n",
    "# CREATING TRAINING DATA\n",
    "for j in range(len(training_datetime)):\n",
    "    current_training_set = training_datetime[j]\n",
    "    current_training_df = pd.DataFrame()\n",
    "    for each_set in current_training_set:\n",
    "        current_training_df = current_training_df.append(raw_data.loc[raw_data['recorded'].str.contains(each_set)])\n",
    "    training_data[j] = current_training_df\n",
    "\n",
    "# CREATE VALIDATION DATA\n",
    "for v in range(len(validation_datetime)):\n",
    "    current_validation_set = validation_datetime[v]\n",
    "    current_validation_df = pd.DataFrame()\n",
    "    for each_set in current_training_set:\n",
    "        current_validation_df = current_validation_df.append(raw_data.loc[raw_data['recorded'].str.contains(each_set)])\n",
    "    validation_data[v] = current_validation_df\n",
    "\n",
    "# CREATE TEST DATA\n",
    "for t in range(len(test_datetime)):\n",
    "    current_test_set = test_datetime[t]\n",
    "    current_test_df = pd.DataFrame()\n",
    "    for each_set in current_test_set:\n",
    "        current_test_df = current_test_df.append(raw_data.loc[raw_data['recorded'].str.contains(each_set)])\n",
    "    testing_data[t] = current_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8301b0dd",
   "metadata": {},
   "source": [
    "### Data Windowing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078c1d1b",
   "metadata": {},
   "source": [
    "#### Data Windowing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a081eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift, label_columns=None):\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in\n",
    "                                        enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in\n",
    "                            enumerate(raw_data.iloc[:, 1:].columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack(\n",
    "                [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "                axis=-1)\n",
    "\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        # targets = np.array(data.iloc[:, 0:7:3], dtype=np.float32)\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=self.shift,\n",
    "            shuffle=False,\n",
    "            batch_size=1,)\n",
    "\n",
    "        ds = ds.map(self.split_window)\n",
    "\n",
    "        return ds\n",
    "\n",
    "    def create_tf_dataset(self, data):\n",
    "        ds = tf.data.Dataset.from_tensors([])\n",
    "        ds = None\n",
    "        for d in data:\n",
    "            if ds == None:\n",
    "                ds = self.make_dataset(data=d.iloc[:, 1:])\n",
    "            else:\n",
    "                ds = ds.concatenate(self.make_dataset(data=d.iloc[:, 1:]))\n",
    "        return ds\n",
    "\n",
    "    def train(self, training_data):\n",
    "        return self.create_tf_dataset(training_data)\n",
    "\n",
    "    def val(self, validation_data):\n",
    "        return self.create_tf_dataset(validation_data)\n",
    "\n",
    "    def test(self, testing_data):\n",
    "        return self.create_tf_dataset(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dd8a47",
   "metadata": {},
   "source": [
    "#### Windowing Training, Validation and Testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6366a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = WindowGenerator(input_width=2, label_width=1, shift=1\n",
    ", label_columns=['ds1', 'ds2', 'ds3'])\n",
    "\n",
    "training_data_tf = window.train(list(training_data.values()))\n",
    "validation_data_tf = window.val(list(validation_data.values()))\n",
    "testing_data_tf = window.test(list(testing_data.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e907e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = []\n",
    "targets = []\n",
    "for train, target in training_data_tf:\n",
    "    a = tf.reshape(train, -1)\n",
    "    b = tf.reshape(target, -1)\n",
    "    inputs.append(a.numpy())\n",
    "    targets.append(b.numpy())\n",
    "\n",
    "inputs = np.array(inputs)\n",
    "targets = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d470d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_val = []\n",
    "targets_val = []\n",
    "for train, target in validation_data_tf:\n",
    "    a = tf.reshape(train, -1)\n",
    "    b = tf.reshape(target, -1)\n",
    "    inputs_val.append(a.numpy())\n",
    "    targets_val.append(b.numpy())\n",
    "\n",
    "inputs_val = np.array(inputs_val)\n",
    "targets_val = np.array(targets_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8926b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test = []\n",
    "targets_test = []\n",
    "for train, target in testing_data_tf:\n",
    "    a = tf.reshape(train, -1)\n",
    "    b = tf.reshape(target, -1)\n",
    "    inputs_test.append(a.numpy())\n",
    "    targets_test.append(b.numpy())\n",
    "\n",
    "inputs_test_mlp = np.array(inputs_test)\n",
    "targets_test_mlp = np.array(targets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7b5791",
   "metadata": {},
   "source": [
    "## TRAINING MODEL AND DEVELOPING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba7de46",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96a94e1",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aea04db",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "targets = []\n",
    "for train, target in training_data_tf:\n",
    "    a = tf.reshape(train, [2, 15])\n",
    "    b = tf.reshape(target, -1)\n",
    "    inputs.append(a.numpy())\n",
    "    targets.append(b.numpy())\n",
    "\n",
    "inputs = np.array(inputs)\n",
    "targets = np.array(targets)\n",
    "\n",
    "inputs_val = []\n",
    "targets_val = []\n",
    "for train, target in validation_data_tf:\n",
    "    a = tf.reshape(train, [2, 15])\n",
    "    b = tf.reshape(target, -1)\n",
    "    inputs_val.append(a.numpy())\n",
    "    targets_val.append(b.numpy())\n",
    "\n",
    "inputs_val = np.array(inputs_val)\n",
    "targets_val = np.array(targets_val)\n",
    "\n",
    "inputs_test = []\n",
    "targets_test = []\n",
    "for train, target in testing_data_tf:\n",
    "    a = tf.reshape(train, [2, 15])\n",
    "    b = tf.reshape(target, -1)\n",
    "    inputs_test.append(a.numpy())\n",
    "    targets_test.append(b.numpy())\n",
    "\n",
    "inputs_test = np.array(inputs_test)\n",
    "targets_test = np.array(targets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323594a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    for i in range(50):\n",
    "        cnn_model = tf.keras.models.Sequential()\n",
    "\n",
    "        # FIRST LAYER\n",
    "        cnn_model.add(tf.keras.layers.Conv1D(filters=60, kernel_size=1, activation='relu', input_shape=(2,15)))\n",
    "        cnn_model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
    "        cnn_model.add(tf.keras.layers.Flatten())\n",
    "        cnn_model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "        cnn_model.add(tf.keras.layers.Dense(3))\n",
    "        cnn_model.add(layers.Reshape((3,)))\n",
    "\n",
    "        # COMPILE MODEL\n",
    "        cnn_model.compile(optimizer=tf.optimizers.Adam(), loss=tf.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "        history = cnn_model.fit(inputs, targets, epochs=50, shuffle=False, validation_data=(inputs_val, targets_val))\n",
    "\n",
    "        result = cnn_model.evaluate(inputs_test, targets_test)\n",
    "        print(f\"Run: {i} times\")\n",
    "        print(\"-------------------------------\")\n",
    "        results.append(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29244c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('cnn_without_result_mse.txt', results)\n",
    "\n",
    "cnn_model.save(\"cnn_model_with_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269e0bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING TRAINING DATA\n",
    "for j in range(len(training_datetime)):\n",
    "    current_training_set = training_datetime[j]\n",
    "    current_training_df = pd.DataFrame()\n",
    "    for each_set in current_training_set:\n",
    "        current_training_df = current_training_df.append(raw_data.loc[raw_data['recorded'].str.contains(each_set)])\n",
    "    training_data[j] = current_training_df\n",
    "\n",
    "# CREATE VALIDATION DATA\n",
    "for v in range(len(validation_datetime)):\n",
    "    current_validation_set = validation_datetime[v]\n",
    "    current_validation_df = pd.DataFrame()\n",
    "    for each_set in current_training_set:\n",
    "        current_validation_df = current_validation_df.append(raw_data.loc[raw_data['recorded'].str.contains(each_set)])\n",
    "    validation_data[v] = current_validation_df\n",
    "\n",
    "# CREATE TEST DATA\n",
    "for t in range(len(test_datetime)):\n",
    "    current_test_set = test_datetime[t]\n",
    "    current_test_df = pd.DataFrame()\n",
    "    for each_set in current_test_set:\n",
    "        current_test_df = current_test_df.append(raw_data.loc[raw_data['recorded'].str.contains(each_set)])\n",
    "    testing_data[t] = current_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c67488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift, label_columns=None):\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in\n",
    "                                        enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in\n",
    "                            enumerate(raw_data.iloc[:, 1:].columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, 2:]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack(\n",
    "                [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "                axis=-1)\n",
    "\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        # targets = np.array(data.iloc[:, 0:7:3], dtype=np.float32)\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=self.shift,\n",
    "            shuffle=False,\n",
    "            batch_size=32,)\n",
    "\n",
    "        ds = ds.map(self.split_window)\n",
    "\n",
    "        return ds\n",
    "\n",
    "    def create_tf_dataset(self, data):\n",
    "        ds = tf.data.Dataset.from_tensors([])\n",
    "        ds = None\n",
    "        for d in data:\n",
    "            if ds == None:\n",
    "                ds = self.make_dataset(data=d.iloc[:, 1:])\n",
    "            else:\n",
    "                ds = ds.concatenate(self.make_dataset(data=d.iloc[:, 1:]))\n",
    "        return ds\n",
    "\n",
    "    def train(self, training_data):\n",
    "        return self.create_tf_dataset(training_data)\n",
    "\n",
    "    def val(self, validation_data):\n",
    "        return self.create_tf_dataset(validation_data)\n",
    "\n",
    "    def test(self, testing_data):\n",
    "        return self.create_tf_dataset(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efcc88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = WindowGenerator(input_width=2, label_width=1, shift=1, label_columns=['ds1', 'ds2', 'ds3'])\n",
    "\n",
    "training_data_tf = window.train(list(training_data.values()))\n",
    "validation_data_tf = window.val(list(validation_data.values()))\n",
    "testing_data_tf = window.test(list(testing_data.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00716bf",
   "metadata": {},
   "source": [
    "#### VANILLA RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8827ef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    for i in range(50):\n",
    "        rnn_model = tf.keras.models.Sequential()\n",
    "\n",
    "        # FIRST LAYER\n",
    "        rnn_model.add(layers.SimpleRNN(units=64, return_sequences = True))\n",
    "        rnn_model.add(layers.Dropout(0.5))\n",
    "\n",
    "        # SECOND LAYER\n",
    "        rnn_model.add(layers.SimpleRNN(units=32, return_sequences = True))\n",
    "        rnn_model.add(layers.Dropout(0.5))\n",
    "\n",
    "        # THIRD LAYER\n",
    "        rnn_model.add(layers.SimpleRNN(units=32, return_sequences = True))\n",
    "        rnn_model.add(layers.Dropout(0.5))\n",
    "\n",
    "\n",
    "        # FORTH LAYER\n",
    "        rnn_model.add(layers.SimpleRNN(units=32))\n",
    "        rnn_model.add(layers.Dropout(0.5))\n",
    "\n",
    "\n",
    "        # OUTPUT LAYER\n",
    "        rnn_model.add(layers.Dense(units=3))\n",
    "        rnn_model.add(layers.Reshape((1, 3)))\n",
    "\n",
    "        # COMPILE MODEL\n",
    "        rnn_model.compile(optimizer=tf.optimizers.Adam(), loss=tf.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "        history = rnn_model.fit(training_data_tf, epochs=50, shuffle=False, validation_data=validation_data_tf)\n",
    "\n",
    "        result = rnn_model.evaluate(testing_data_tf)\n",
    "        print(f\"Run: {i} times\")\n",
    "        print(\"-------------------------------\")\n",
    "        results.append(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94577d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('rnn_without_result_mse.txt', results)\n",
    "\n",
    "rnn_model.save(\"rnn_model_with_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705e9cf3",
   "metadata": {},
   "source": [
    "#### LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8328f409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = []\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "#     for i in range(50):\n",
    "        lstm_model = tf.keras.models.Sequential()\n",
    "\n",
    "        # FIRST LAYER\n",
    "        lstm_model.add(layers.LSTM(units=64, bias_initializer=tf.keras.initializers.HeNormal(), return_sequences = True))\n",
    "        lstm_model.add(layers.Dropout(0.5))\n",
    "\n",
    "        # SECOND LAYER\n",
    "        lstm_model.add(layers.LSTM(units=32, bias_initializer=tf.keras.initializers.HeNormal(), return_sequences = True))\n",
    "        lstm_model.add(layers.Dropout(0.5))\n",
    "\n",
    "        # THIRD LAYER\n",
    "        lstm_model.add(layers.LSTM(units=32, bias_initializer=tf.keras.initializers.HeNormal(), return_sequences = True))\n",
    "        lstm_model.add(layers.Dropout(0.5))\n",
    "\n",
    "        # FORTH LAYER\n",
    "        lstm_model.add(layers.LSTM(units=32, bias_initializer=tf.keras.initializers.HeNormal()))\n",
    "        lstm_model.add(layers.Dropout(0.5))\n",
    "\n",
    "        # OUTPUT LAYER\n",
    "        lstm_model.add(layers.Dense(units=3))\n",
    "        lstm_model.add(layers.Reshape((1, 3)))\n",
    "\n",
    "        # COMPILE MODEL\n",
    "        lstm_model.compile(optimizer=tf.optimizers.Adam(), loss=tf.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "        history = lstm_model.fit(training_data_tf, epochs=100, shuffle=False, validation_data=validation_data_tf)\n",
    "        ds_predicted_evaluation = lstm_model.evaluate(testing_data_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba78081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = np.array([0.0352,0.0464,0.0673,0.1275,0.1624,0.1958,0.1802,0.1894])\n",
    "mse = np.array([0.0022,0.0042,0.0089,0.0274,0.0407,0.0582,0.0464,0.0501])\n",
    "\n",
    "np.savetxt('maes.txt', mae, delimiter=',')\n",
    "np.savetxt('mses.txt', mse, delimiter=',')\n",
    "\n",
    "np.savetxt('lstm_without_result_mse.txt', results)\n",
    "\n",
    "lstm_model.save(\"lstm_model_with_text\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
