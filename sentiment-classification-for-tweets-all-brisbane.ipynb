{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from datetime import date, timedelta, datetime\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "%matplotlib qt\n",
    "# %install_ext https://raw.github.com/cpcloud/ipython-autotime/master/autotime.py\n",
    "# %load_ext autotime\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = pd.read_csv('dataset/tweets_all_brisbane.csv')\n",
    "all_tweets.drop(columns='Unnamed: 0', inplace=True)\n",
    "all_tweets['date'] = pd.to_datetime(all_tweets['date'])\n",
    "all_tweets.sort_values(by='date', inplace=True, ignore_index=True)\n",
    "all_tweets['date'] = all_tweets['date'].dt.strftime('%#d/%m/%Y %H:%M')\n",
    "all_tweets.drop_duplicates(inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combi = all_tweets\n",
    "combi.columns = ['date', 'tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REMOVING TWITTER MENTION (@user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt\n",
    "\n",
    "# remove twitter handles (@user)\n",
    "combi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REMOVING URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(df):\n",
    "    df['tidy_tweet'] = df['tidy_tweet'].str.replace(r\"http\\S+\", \"\")\n",
    "\n",
    "remove_urls(combi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REMOVING PUNCTATION, NUMBERS AND SPECIAL CHARACTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters, numbers, punctuations\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REMOVING SHORT WORDS AND LOWER CASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove short words (length <= 3)\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# lower case\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REMOVING STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = stopwords.words('english')\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda text: cleaning_stopwords(text))\n",
    "combi['tidy_tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split())\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "\n",
    "combi['tidy_tweet'] = tokenized_tweet\n",
    "combi.columns = ['date', 'tweet', 'tidy_tweet']\n",
    "\n",
    "key_words = ['covid', 'locldown', 'cold', 'rain', 'hot', 'congestion', 'traffic', 'road']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame()\n",
    "\n",
    "for key_word in key_words:\n",
    "    temp = combi.loc[combi['tidy_tweet'].str.contains(key_word), :]\n",
    "    new_df = new_df.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.style.set_properties(subset=['tweet'], **{'width': '1000px'}, inplace=True)\n",
    "new_df[['date', 'tweet']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import option_context\n",
    "\n",
    "with option_context('display.max_colwidth', 300):\n",
    "    display(new_df[['date', 'tweet']].head(5).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(cat for cat in new_df.tidy_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "\n",
    "mask = np.array(Image.open('aus.png'))\n",
    "wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='white', colormap='tab10', collocations=False, stopwords = STOPWORDS, mask=mask).generate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot word cloud\n",
    "def plot_cloud(wordcloud):\n",
    "    # Set figure size\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    # Display image\n",
    "    plt.imshow(wordcloud) \n",
    "    # No axis details\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SENTIMENT CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "model = load('sentiment-SVM-model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df = combi\n",
    "X = selected_df.loc[:, 'tidy_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=10000)\n",
    "X = vectoriser.fit_transform(X)\n",
    "X = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "result['date'] = combi['date']\n",
    "result['sentiment'] = pd.DataFrame(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('dataset/tweets_all_brisbane_sentiment_done.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COUNTING POS/NEG TWEETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimented_tweets = pd.read_csv('dataset/tweets_all_brisbane_sentiment_done.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE DATE RANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdate = pd.to_datetime('2021-02-21')\n",
    "edate = pd.to_datetime('2021-06-19')\n",
    "all_dates = pd.date_range(sdate,edate-timedelta(days=1),freq='D')\n",
    "all_dates = all_dates.strftime('%#d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_range(start, end, delta):\n",
    "    current = start\n",
    "    while current < end:\n",
    "        yield current\n",
    "        current += delta\n",
    "\n",
    "min_intervals = [dt.strftime('%H:%M') for dt in \n",
    "       datetime_range(datetime(2021, 2, 20, 23,59), datetime(2021, 2, 22, 0,0,0), \n",
    "       timedelta(minutes=5))]\n",
    "\n",
    "min_intervals = min_intervals[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COUNTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tweets_df = pd.DataFrame()\n",
    "current_time = '20/02/2021 23:59'\n",
    "for date in all_dates:\n",
    "    all_daily_tweets = sentimented_tweets.loc[sentimented_tweets['date'].str.contains(date), :]\n",
    "    for intervel in min_intervals:\n",
    "        tweets_in_interval = all_daily_tweets.loc[(pd.to_datetime(all_daily_tweets['date'], format='%d/%m/%Y %H:%M') > pd.to_datetime(f'{current_time}', format='%d/%m/%Y %H:%M')) & (pd.to_datetime(all_daily_tweets['date'], format='%d/%m/%Y %H:%M') <= pd.to_datetime(f'{date} {intervel}', format='%d/%m/%Y %H:%M')), :]\n",
    "        neg = len(tweets_in_interval.loc[tweets_in_interval['sentiment'] == 0, :])\n",
    "        pos = len(tweets_in_interval.loc[tweets_in_interval['sentiment'] == 1, :])\n",
    "        count_tweets_df = count_tweets_df.append(pd.DataFrame([f\"{date} {intervel}\", neg, pos]).T)\n",
    "        current_time = f\"{date} {intervel}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tweets_df.columns = ['date', 'num_neg', 'num_pos']\n",
    "\n",
    "count_tweets_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "count_tweets_df.to_csv('dataset/count_sentiment_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MERGE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data = pd.read_csv('dataset/count_sentiment_tweets.csv')\n",
    "traffic_data = pd.read_csv('dataset/acceptable_traffic_data_processed.csv')\n",
    "\n",
    "traffic_data.sort_values(by='recorded', inplace=True, ignore_index=True)\n",
    "\n",
    "traffic_data['recorded'] = pd.to_datetime(traffic_data['recorded']).apply(lambda x: x.strftime('%#d/%m/%Y %H:%M'))\n",
    "traffic_data[['num_neg', 'num_pos']] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates = traffic_data['recorded'].to_numpy()\n",
    "satisfied_df = tweets_data[tweets_data['date'].isin(all_dates)]\n",
    "satisfied_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satisfied_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_data.loc[:, ['num_neg', 'num_pos']] = satisfied_df[['num_neg', 'num_pos']]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15c4639e63f425d407e543b89ba5bed6f4556646905044daa4286140c7b13f14"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
