{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d46b668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b33b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('dataset/traffic_label_acceptable_traffic_data_new.csv')\n",
    "\n",
    "day_of_week = pd.date_range('21/02/2021', '08/07/2021', freq='D').to_series().dt.dayofweek\n",
    "raw_data['weekday'] = 0\n",
    "\n",
    "datetimes = raw_data['recorded'].to_numpy()\n",
    "days = np.core.defchararray.split(datetimes.astype(str), sep=' ')\n",
    "\n",
    "unique_dates = dict()\n",
    "\n",
    "for day in days:\n",
    "    if unique_dates.get(day[0]) == None:\n",
    "        unique_dates[day[0]] = True\n",
    "\n",
    "valid_days = list(unique_dates.keys())\n",
    "\n",
    "for day in unique_dates:\n",
    "    raw_data.loc[raw_data['recorded'].str.contains(day), 'weekday'] = day_of_week[day]\n",
    "    \n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "raw_data[['ds1', 'mf1', 'rf1', 'ds2', 'mf2', 'rf2', 'ds3', 'mf3', 'rf3', 'num_neg', 'num_pos']] = scaler.fit_transform(raw_data[['ds1', 'mf1', 'rf1', 'ds2', 'mf2', 'rf2', 'ds3', 'mf3', 'rf3', 'num_neg', 'num_pos']])\n",
    "\n",
    "raw_data['nextday_weekday'] = 0\n",
    "raw_data['nextday_weekend'] = 0\n",
    "raw_data['nextday_holiday'] = 0\n",
    "\n",
    "old_holiday = ('4/2/2021', '4/3/2021', '4/4/2021', '4/5/2021', '4/26/2021', '4/25/2021', '3/30/2021', '3/31/2021', '4/1/2021')\n",
    "holiday = set()\n",
    "\n",
    "for day in old_holiday:\n",
    "    date = pd.to_datetime(day, format='%m/%d/%Y')\n",
    "    holiday.add(date.strftime('%Y-%m-%d'))\n",
    "    \n",
    "for day in unique_dates.keys():\n",
    "    if (day_of_week[day] + 1) % 7 == 5 or (day_of_week[day] + 1) % 7 == 6:\n",
    "        raw_data.loc[raw_data['recorded'].str.contains(day), 'nextday_weekend'] = 1\n",
    "    \n",
    "    if (day_of_week[day] + 1) % 7 >= 0 and (day_of_week[day] + 1) % 7 <= 4:\n",
    "        raw_data.loc[raw_data['recorded'].str.contains(day), 'nextday_weekday'] = 1\n",
    "\n",
    "    if (pd.to_datetime(day) + dt.timedelta(days=1)).strftime('%Y-%m-%d') in holiday:\n",
    "        raw_data.loc[raw_data['recorded'].str.contains(day), 'nextday_holiday'] = 1\n",
    "        \n",
    "datetime = pd.to_datetime(np.array(list(unique_dates.keys()))).sort_values().strftime('%Y-%m-%d')\n",
    "datetime = datetime.drop('2021-06-11')\n",
    "\n",
    "training_datetime = np.array([['2021-02-21', '2021-02-22', '2021-02-23', '2021-02-24', '2021-02-25', '2021-02-26', '2021-02-27', '2021-02-28', '2021-03-01', '2021-03-02', '2021-03-03'],\n",
    "       ['2021-03-12', '2021-03-13', '2021-03-14', '2021-03-15', '2021-03-16', '2021-03-17', '2021-03-18', '2021-03-19', '2021-03-20', '2021-03-21', '2021-03-22', '2021-03-23'],\n",
    "       ['2021-03-26', '2021-03-27', '2021-03-28', '2021-03-29'], \n",
    "       ['2021-04-04', '2021-04-05', '2021-04-06', '2021-04-07', '2021-04-08', '2021-04-09'],\n",
    "       ['2021-04-13', '2021-04-14', '2021-04-15', '2021-04-16', '2021-04-17', '2021-04-18'],\n",
    "       ['2021-04-23', '2021-04-24', '2021-04-25', '2021-04-26', '2021-04-27', '2021-04-28', '2021-04-29', '2021-04-30', '2021-05-01'],\n",
    "       ['2021-05-04', '2021-05-05', '2021-05-06', '2021-05-07']], dtype=object)\n",
    "\n",
    "validation_datetime = np.array([['2021-05-08', '2021-05-09', '2021-05-10', '2021-05-11', '2021-05-12', '2021-05-13', '2021-05-14'],\n",
    "        ['2021-05-21', '2021-05-22', '2021-05-23', '2021-05-24', '2021-05-25', '2021-05-26', '2021-05-27'],\n",
    "        ['2021-06-01', '2021-06-02', '2021-06-03']], dtype=object)\n",
    "\n",
    "test_datetime = np.array([['2021-06-04', '2021-06-05', '2021-06-06', '2021-06-07'], \n",
    "                          ['2021-06-15', '2021-06-16', '2021-06-17'],\n",
    "                         ['2021-03-30', '2021-03-31', '2021-04-01', '2021-04-02', '2021-04-03']], dtype=object)\n",
    "\n",
    "\n",
    "training_data = dict()\n",
    "validation_data = dict()\n",
    "testing_data = dict()\n",
    "\n",
    "# CREATING TRAINING DATA\n",
    "for j in range(len(training_datetime)):\n",
    "    current_training_set = training_datetime[j]\n",
    "    current_training_df = pd.DataFrame()\n",
    "    for each_set in current_training_set:\n",
    "        current_training_df = current_training_df.append(raw_data.loc[raw_data['recorded'].str.contains(each_set)])\n",
    "    training_data[j] = current_training_df\n",
    "\n",
    "# CREATE VALIDATION DATA\n",
    "for v in range(len(validation_datetime)):\n",
    "    current_validation_set = validation_datetime[v]\n",
    "    current_validation_df = pd.DataFrame()\n",
    "    for each_set in current_training_set:\n",
    "        current_validation_df = current_validation_df.append(raw_data.loc[raw_data['recorded'].str.contains(each_set)])\n",
    "    validation_data[v] = current_validation_df\n",
    "\n",
    "# CREATE TEST DATA\n",
    "for t in range(len(test_datetime)):\n",
    "    current_test_set = test_datetime[t]\n",
    "    current_test_df = pd.DataFrame()\n",
    "    for each_set in current_test_set:\n",
    "        current_test_df = current_test_df.append(raw_data.loc[raw_data['recorded'].str.contains(each_set)])\n",
    "    testing_data[t] = current_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d3c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift, label_columns=None):\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in\n",
    "                                        enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in\n",
    "                            enumerate(raw_data.iloc[:, 1:].columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack(\n",
    "                [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "                axis=-1)\n",
    "\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        # targets = np.array(data.iloc[:, 0:7:3], dtype=np.float32)\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=self.shift,\n",
    "            shuffle=False,\n",
    "            batch_size=1,)\n",
    "\n",
    "        ds = ds.map(self.split_window)\n",
    "\n",
    "        return ds\n",
    "\n",
    "    def create_tf_dataset(self, data):\n",
    "        ds = tf.data.Dataset.from_tensors([])\n",
    "        ds = None\n",
    "        for d in data:\n",
    "            if ds == None:\n",
    "                ds = self.make_dataset(data=d.iloc[:, 1:])\n",
    "            else:\n",
    "                ds = ds.concatenate(self.make_dataset(data=d.iloc[:, 1:]))\n",
    "        return ds\n",
    "\n",
    "    def train(self, training_data):\n",
    "        return self.create_tf_dataset(training_data)\n",
    "\n",
    "    def val(self, validation_data):\n",
    "        return self.create_tf_dataset(validation_data)\n",
    "\n",
    "    def test(self, testing_data):\n",
    "        return self.create_tf_dataset(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dd8a47",
   "metadata": {},
   "source": [
    "#### Windowing Training, Validation and Testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6366a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = WindowGenerator(input_width=2, label_width=1, shift=1\n",
    ", label_columns=['ds1', 'ds2', 'ds3'])\n",
    "\n",
    "training_data_tf = window.train(list(training_data.values()))\n",
    "validation_data_tf = window.val(list(validation_data.values()))\n",
    "testing_data_tf = window.test(list(testing_data.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e907e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = []\n",
    "targets = []\n",
    "for train, target in training_data_tf:\n",
    "    a = tf.reshape(train, -1)\n",
    "    b = tf.reshape(target, -1)\n",
    "    inputs.append(a.numpy())\n",
    "    targets.append(b.numpy())\n",
    "\n",
    "inputs = np.array(inputs)\n",
    "targets = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d470d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_val = []\n",
    "targets_val = []\n",
    "for train, target in validation_data_tf:\n",
    "    a = tf.reshape(train, -1)\n",
    "    b = tf.reshape(target, -1)\n",
    "    inputs_val.append(a.numpy())\n",
    "    targets_val.append(b.numpy())\n",
    "\n",
    "inputs_val = np.array(inputs_val)\n",
    "targets_val = np.array(targets_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8926b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test = []\n",
    "targets_test = []\n",
    "for train, target in testing_data_tf:\n",
    "    a = tf.reshape(train, -1)\n",
    "    b = tf.reshape(target, -1)\n",
    "    inputs_test.append(a.numpy())\n",
    "    targets_test.append(b.numpy())\n",
    "\n",
    "inputs_test_mlp = np.array(inputs_test)\n",
    "targets_test_mlp = np.array(targets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aea04db",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "targets = []\n",
    "for train, target in training_data_tf:\n",
    "    a = tf.reshape(train, [2, 15])\n",
    "    b = tf.reshape(target, -1)\n",
    "    inputs.append(a.numpy())\n",
    "    targets.append(b.numpy())\n",
    "\n",
    "inputs = np.array(inputs)\n",
    "targets = np.array(targets)\n",
    "\n",
    "inputs_val = []\n",
    "targets_val = []\n",
    "for train, target in validation_data_tf:\n",
    "    a = tf.reshape(train, [2, 15])\n",
    "    b = tf.reshape(target, -1)\n",
    "    inputs_val.append(a.numpy())\n",
    "    targets_val.append(b.numpy())\n",
    "\n",
    "inputs_val = np.array(inputs_val)\n",
    "targets_val = np.array(targets_val)\n",
    "\n",
    "inputs_test = []\n",
    "targets_test = []\n",
    "for train, target in testing_data_tf:\n",
    "    a = tf.reshape(train, [2, 15])\n",
    "    b = tf.reshape(target, -1)\n",
    "    inputs_test.append(a.numpy())\n",
    "    targets_test.append(b.numpy())\n",
    "\n",
    "inputs_test = np.array(inputs_test)\n",
    "targets_test = np.array(targets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269e0bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING TRAINING DATA\n",
    "for j in range(len(training_datetime)):\n",
    "    current_training_set = training_datetime[j]\n",
    "    current_training_df = pd.DataFrame()\n",
    "    for each_set in current_training_set:\n",
    "        current_training_df = current_training_df.append(raw_data.loc[raw_data['recorded'].str.contains(each_set)])\n",
    "    training_data[j] = current_training_df\n",
    "\n",
    "# CREATE VALIDATION DATA\n",
    "for v in range(len(validation_datetime)):\n",
    "    current_validation_set = validation_datetime[v]\n",
    "    current_validation_df = pd.DataFrame()\n",
    "    for each_set in current_training_set:\n",
    "        current_validation_df = current_validation_df.append(raw_data.loc[raw_data['recorded'].str.contains(each_set)])\n",
    "    validation_data[v] = current_validation_df\n",
    "\n",
    "# CREATE TEST DATA\n",
    "for t in range(len(test_datetime)):\n",
    "    current_test_set = test_datetime[t]\n",
    "    current_test_df = pd.DataFrame()\n",
    "    for each_set in current_test_set:\n",
    "        current_test_df = current_test_df.append(raw_data.loc[raw_data['recorded'].str.contains(each_set)])\n",
    "    testing_data[t] = current_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c67488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift, label_columns=None):\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in\n",
    "                                        enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in\n",
    "                            enumerate(raw_data.iloc[:, 1:].columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, 2:]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack(\n",
    "                [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "                axis=-1)\n",
    "\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        # targets = np.array(data.iloc[:, 0:7:3], dtype=np.float32)\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=self.shift,\n",
    "            shuffle=False,\n",
    "            batch_size=32,)\n",
    "\n",
    "        ds = ds.map(self.split_window)\n",
    "\n",
    "        return ds\n",
    "\n",
    "    def create_tf_dataset(self, data):\n",
    "        ds = tf.data.Dataset.from_tensors([])\n",
    "        ds = None\n",
    "        for d in data:\n",
    "            if ds == None:\n",
    "                ds = self.make_dataset(data=d.iloc[:, 1:])\n",
    "            else:\n",
    "                ds = ds.concatenate(self.make_dataset(data=d.iloc[:, 1:]))\n",
    "        return ds\n",
    "\n",
    "    def train(self, training_data):\n",
    "        return self.create_tf_dataset(training_data)\n",
    "\n",
    "    def val(self, validation_data):\n",
    "        return self.create_tf_dataset(validation_data)\n",
    "\n",
    "    def test(self, testing_data):\n",
    "        return self.create_tf_dataset(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efcc88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = WindowGenerator(input_width=2, label_width=1, shift=1, label_columns=['ds1', 'ds2', 'ds3'])\n",
    "\n",
    "training_data_tf = window.train(list(training_data.values()))\n",
    "validation_data_tf = window.val(list(validation_data.values()))\n",
    "testing_data_tf = window.test(list(testing_data.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15852764",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = tf.keras.models.load_model('cnn_model_with_text')\n",
    "rnn_model = tf.keras.models.load_model('rnn_model_with_text')\n",
    "lstm_model = tf.keras.models.load_model('lstm_model_with_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6b1242",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_predicted_cnn = cnn_model.predict(inputs_test)\n",
    "ds_predicted_rnn =  rnn_model.predict(testing_data_tf)\n",
    "ds_predicted_lstm = lstm_model.predict(testing_data_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a359ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_predicted_cnn.shape)\n",
    "print(ds_predicted_rnn.shape)\n",
    "print(ds_predicted_lstm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6595ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data_df = pd.DataFrame()\n",
    "for i in range(len(testing_data.values())):\n",
    "    testing_data_df = testing_data_df.append(testing_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90850dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data_df_copy_cnn = testing_data_df.copy()\n",
    "testing_data_df_copy_cnn.reset_index(inplace=True)\n",
    "\n",
    "testing_data_df_copy_rnn = testing_data_df.copy()\n",
    "testing_data_df_copy_rnn.reset_index(inplace=True)\n",
    "\n",
    "testing_data_df_copy_lstm = testing_data_df.copy()\n",
    "testing_data_df_copy_lstm.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0010cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_predicted_cnn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a370eb6d",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5872ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSIGN DS1\n",
    "testing_data_df_copy_cnn.loc[2:187, 'ds1'] = ds_predicted_cnn[:, 0].flatten()[:186]\n",
    "testing_data_df_copy_cnn.loc[190:328, 'ds1'] = ds_predicted_cnn[:, 0].flatten()[186:325]\n",
    "testing_data_df_copy_cnn.loc[331:, 'ds1'] =  ds_predicted_cnn[:, 0].flatten()[325:]\n",
    "# testing_data_df_copy_cnn.loc[491:, 'ds1'] =  ds_predicted_cnn[:, 0].flatten()[483:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754a8caa",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e24c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ASSIGN DS1\n",
    "# testing_data_df_copy_rnn.loc[2:187, 'ds1'] = ds_predicted_rnn[:, :, 0].flatten()[:186]\n",
    "# testing_data_df_copy_rnn.loc[190:328, 'ds1'] = ds_predicted_rnn[:, :, 0].flatten()[186:325]\n",
    "# testing_data_df_copy_rnn.loc[331:, 'ds1'] =  ds_predicted_rnn[:, :, 0].flatten()[325:]\n",
    "\n",
    "# # # ASSIGN DS2\n",
    "# testing_data_df_copy_rnn.loc[2:187, 'ds2'] = ds_predicted_rnn[:, :, 0].flatten()[:186]\n",
    "# testing_data_df_copy_rnn.loc[190:328, 'ds2'] = ds_predicted_rnn[:, :, 0].flatten()[186:325]\n",
    "# testing_data_df_copy_rnn.loc[331:, 'ds2'] =  ds_predicted_rnn[:, :, 0].flatten()[325:]\n",
    "\n",
    "# # # ASSIGN DS3\n",
    "# testing_data_df_copy_rnn.loc[2:187, 'ds3'] = ds_predicted_rnn[:, :, 0].flatten()[:186]\n",
    "# testing_data_df_copy_rnn.loc[190:328, 'ds3'] = ds_predicted_rnn[:, :, 0].flatten()[186:325]\n",
    "# testing_data_df_copy_rnn.loc[331:, 'ds3'] =  ds_predicted_rnn[:, :, 0].flatten()[325:]\n",
    "\n",
    "# ASSIGN DS1\n",
    "testing_data_df_copy_rnn.loc[2:187, 'ds1'] = ds_predicted_rnn[:, :, 0].flatten()[:186]\n",
    "testing_data_df_copy_rnn.loc[190:328, 'ds1'] = ds_predicted_rnn[:, :, 0].flatten()[186:325]\n",
    "testing_data_df_copy_rnn.loc[331:, 'ds1'] =  ds_predicted_rnn[:, :, 0].flatten()[325:]\n",
    "# testing_data_df_copy_rnn.loc[491:, 'ds1'] =  ds_predicted_rnn[:, :, 0].flatten()[483:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0342ea2",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d75308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ASSIGN DS1\n",
    "# testing_data_df_copy_lstm.loc[2:187, 'ds1'] = ds_predicted_lstm[:, :, 0].flatten()[:186]\n",
    "# testing_data_df_copy_lstm.loc[190:328, 'ds1'] = ds_predicted_lstm[:, :, 0].flatten()[186:325]\n",
    "# testing_data_df_copy_lstm.loc[331:, 'ds1'] =  ds_predicted_lstm[:, :, 0].flatten()[325:]\n",
    "\n",
    "# # # ASSIGN DS2\n",
    "# testing_data_df_copy_lstm.loc[2:187, 'ds2'] = ds_predicted_lstm[:, :, 0].flatten()[:186]\n",
    "# testing_data_df_copy_lstm.loc[190:328, 'ds2'] = ds_predicted_lstm[:, :, 0].flatten()[186:325]\n",
    "# testing_data_df_copy_lstm.loc[331:, 'ds2'] =  ds_predicted_lstm[:, :, 0].flatten()[325:]\n",
    "\n",
    "# # # ASSIGN DS3\n",
    "# testing_data_df_copy_lstm.loc[2:187, 'ds3'] = ds_predicted_lstm[:, :, 0].flatten()[:186]\n",
    "# testing_data_df_copy_lstm.loc[190:328, 'ds3'] = ds_predicted_lstm[:, :, 0].flatten()[186:325]\n",
    "# testing_data_df_copy_lstm.loc[331:, 'ds3'] =  ds_predicted_lstm[:, :, 0].flatten()[325:]\n",
    "\n",
    "# ASSIGN DS1\n",
    "testing_data_df_copy_lstm.loc[2:187, 'ds1'] = ds_predicted_lstm[:, :, 0].flatten()[:186]\n",
    "testing_data_df_copy_lstm.loc[190:328, 'ds1'] = ds_predicted_lstm[:, :, 0].flatten()[186:325]\n",
    "testing_data_df_copy_lstm.loc[331:, 'ds1'] =  ds_predicted_lstm[:, :, 0].flatten()[325:]\n",
    "# testing_data_df_copy_lstm.loc[491:, 'ds1'] =  ds_predicted_lstm[:, :, 0].flatten()[483:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439b2ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data_df_copy_cnn[['ds1', 'mf1', 'rf1', 'ds2', 'mf2', 'rf2', 'ds3', 'mf3', 'rf3', 'num_neg', 'num_pos']] = scaler.inverse_transform(testing_data_df_copy_cnn[['ds1', 'mf1', 'rf1', 'ds2', 'mf2', 'rf2', 'ds3', 'mf3', 'rf3', 'num_neg', 'num_pos']])\n",
    "testing_data_df_copy_cnn.drop(columns='index', inplace=True)\n",
    "\n",
    "testing_data_df_copy_rnn[['ds1', 'mf1', 'rf1', 'ds2', 'mf2', 'rf2', 'ds3', 'mf3', 'rf3', 'num_neg', 'num_pos']] = scaler.inverse_transform(testing_data_df_copy_rnn[['ds1', 'mf1', 'rf1', 'ds2', 'mf2', 'rf2', 'ds3', 'mf3', 'rf3', 'num_neg', 'num_pos']])\n",
    "testing_data_df_copy_rnn.drop(columns='index', inplace=True)\n",
    "\n",
    "testing_data_df_copy_lstm[['ds1', 'mf1', 'rf1', 'ds2', 'mf2', 'rf2', 'ds3', 'mf3', 'rf3', 'num_neg', 'num_pos']] = scaler.inverse_transform(testing_data_df_copy_lstm[['ds1', 'mf1', 'rf1', 'ds2', 'mf2', 'rf2', 'ds3', 'mf3', 'rf3', 'num_neg', 'num_pos']])\n",
    "testing_data_df_copy_lstm.drop(columns='index', inplace=True)\n",
    "\n",
    "# REVERSE SCALING DATA FOR GROUTH-TRUTH\n",
    "testing_data_df.reset_index(inplace=True)\n",
    "testing_data_df[['ds1', 'mf1', 'rf1', 'ds2', 'mf2', 'rf2', 'ds3', 'mf3', 'rf3', 'num_neg', 'num_pos']] = scaler.inverse_transform(testing_data_df[['ds1', 'mf1', 'rf1', 'ds2', 'mf2', 'rf2', 'ds3', 'mf3', 'rf3', 'num_neg', 'num_pos']])\n",
    "testing_data_df.drop(columns='index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951caecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1_tests = [testing_data_df.loc[2:140, 'ds1'], testing_data_df.loc[190:328, 'ds1'], testing_data_df.loc[331:, 'ds1']]\n",
    "ds1_preds_cnn = [testing_data_df_copy_cnn.loc[2:140, 'ds1'], testing_data_df_copy_cnn.loc[190:328, 'ds1'], testing_data_df_copy_cnn.loc[331:, 'ds1']]\n",
    "ds1_preds_rnn = [testing_data_df_copy_rnn.loc[2:140, 'ds1'], testing_data_df_copy_rnn.loc[190:328, 'ds1'], testing_data_df_copy_rnn.loc[331:, 'ds1']]\n",
    "ds1_preds_lstm = [testing_data_df_copy_lstm.loc[2:140, 'ds1'], testing_data_df_copy_lstm.loc[190:328, 'ds1'], testing_data_df_copy_lstm.loc[331:, 'ds1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddd4646",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds1_tests = [testing_data_df.loc[2:140, 'ds1'], testing_data_df.loc[190:328, 'ds1'], testing_data_df.loc[331:, 'ds1']]\n",
    "ds1_preds_cnn = [testing_data_df_copy_cnn.loc[2:140, 'ds1'], testing_data_df_copy_cnn.loc[190:328, 'ds1'], testing_data_df_copy_cnn.loc[331:, 'ds1']]\n",
    "ds1_preds_rnn = [testing_data_df_copy_rnn.loc[2:140, 'ds1'], testing_data_df_copy_rnn.loc[190:328, 'ds1'], testing_data_df_copy_rnn.loc[331:, 'ds1']]\n",
    "ds1_preds_lstm = [testing_data_df_copy_lstm.loc[2:140, 'ds1'], testing_data_df_copy_lstm.loc[190:328, 'ds1'], testing_data_df_copy_lstm.loc[331:, 'ds1']]\n",
    "\n",
    "test_date = testing_data_df_copy_cnn.loc[2:140, 'recorded']\n",
    "visualize_dates = [pd.to_datetime(testing_data_df_copy_cnn.loc[2:140, 'recorded'].to_numpy()), pd.to_datetime(testing_data_df_copy_cnn.loc[190:328, 'recorded'].to_numpy()), pd.to_datetime(testing_data_df_copy_cnn.loc[331:, 'recorded'].to_numpy())]\n",
    "\n",
    "visualize_date = pd.to_datetime(test_date.to_numpy())\n",
    "days = mdates.DayLocator()\n",
    "formatter = mdates.DateFormatter('%Y-%m-%d %H:%M')\n",
    "\n",
    "fig, axs = plt.subplots(3)\n",
    "name = ['WEEKDAY', 'WEEKEND', 'LOCKDOWN + HOLIDAY']\n",
    "\n",
    "fig.set_size_inches(25, 15, forward=True)\n",
    "fig.autofmt_xdate()\n",
    "for i in range(len(axs)):\n",
    "    date_min = visualize_dates[i][0]\n",
    "    date_max = visualize_dates[i][-1]\n",
    "    axs[i].xaxis.set_major_locator(days)\n",
    "    axs[i].xaxis.set_major_formatter(formatter)\n",
    "    axs[i].set_title(name[i], size=30)\n",
    "    axs[i].set_xlim(date_min, date_max)\n",
    "    axs[i].format_xdata = mdates.DateFormatter('%Y-%m-%d %H:%M')\n",
    "    axs[i].plot(visualize_dates[i], ds1_preds_cnn[i])\n",
    "    axs[i].plot(visualize_dates[i], ds1_preds_rnn[i])\n",
    "    axs[i].plot(visualize_dates[i], ds1_preds_lstm[i])\n",
    "    axs[i].plot(visualize_dates[i], ds1_tests[i])\n",
    "    axs[i].tick_params(axis=\"x\", labelsize=25)\n",
    "    axs[i].tick_params(axis=\"y\", labelsize=40)\n",
    "\n",
    "fig.legend(['CNN prediction', 'RNN prediction', 'LSTM prediction', 'Ground-truth predition'], loc='upper center', ncol=4, prop={'size': 30})\n",
    "\n",
    "lines = fig.legend(['CNN prediction', 'RNN prediction', 'LSTM prediction', 'Ground-truth predition'], loc='upper center', ncol=4, prop={'size': 30})\n",
    "for line in lines.get_lines():\n",
    "    line.set_linewidth(10.0)\n",
    "    \n",
    "# plt.savefig('prediction.png', dpi=300, bbox_inches = 'tight', pad_inches = 0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
